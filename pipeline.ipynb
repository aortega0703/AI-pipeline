{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preliminars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy as sci\n",
    "from scipy import stats\n",
    "from scipy.special import expit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy.linalg import inv\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools as iter\n",
    "%matplotlib widget\n",
    "\n",
    "cluster = True\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_axis_labels(ax, x, y, z=None):\n",
    "    ax.set_xlabel(x)\n",
    "    ax.set_ylabel(y)\n",
    "    if z != None:\n",
    "        ax.set_zlabel(z)\n",
    "\n",
    "\n",
    "def plot_cluster(X, U, Ud=None, C=None,\n",
    "                 title=\"Clusters\",\n",
    "                 axes_names=[\"$X_0$\", \"$X_1$\", \"$Y_0$\"],\n",
    "                 cluster_names=None):\n",
    "    C_N= U.shape[0]\n",
    "    compare = Ud is not None\n",
    "    error = 0\n",
    "\n",
    "    cmap = plt.get_cmap(\"viridis\")\n",
    "    cnorm = mpl.colors.Normalize(vmin=0, vmax=C_N-1)\n",
    "    plt.figure()\n",
    "    ax = plt.axes(projection=\"3d\", title=title)\n",
    "    \n",
    "    U = np.argmax(U, axis=0)\n",
    "    Ud = np.argmax(Ud, axis=0) if compare else U\n",
    "    if cluster_names is None:\n",
    "        cluster_names = [f\"C{c}\" for c in range(C_N)]\n",
    "    for c in range(C_N):\n",
    "        x = U == c\n",
    "        xT = x & (Ud == c)\n",
    "        xF = x & (Ud != c)\n",
    "        c_name = cluster_names[c] + f\": {np.sum(xT)}\"\n",
    "        if compare:\n",
    "            e_c = np.sum(xF)\n",
    "            c_name += f\" - {e_c}\"\n",
    "            error += e_c\n",
    "        ax.scatter(*X[:3, xT], color=cmap(cnorm(c)), label=c_name)\n",
    "        ax.scatter(*X[:3, xF], color=cmap(cnorm(c)), marker=\"x\")\n",
    "    \n",
    "    set_axis_labels(ax, *axes_names)\n",
    "    if compare:\n",
    "        ax.scatter([], [], [], color='k', marker=\"x\", label=f\"Errors: {error}\")\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Misc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(points):\n",
    "    min = points.min(axis=1, keepdims=True)\n",
    "    scale = points.max(axis=1, keepdims=True) - min\n",
    "    def denormalize(points): return points * scale + min\n",
    "    return (points - min) / scale, denormalize\n",
    "\n",
    "def name_to_int(data):\n",
    "    names = data.unique()\n",
    "    name_map = dict(map(reversed, enumerate(names)))\n",
    "    return data.apply(lambda x: name_map[x]), names\n",
    "\n",
    "def vectorize(data):\n",
    "    I = np.identity(np.max(data) + 1)\n",
    "    return I[:, data]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Norms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_names = [\n",
    "    \"Euclidean2\",\n",
    "    \"Manhattan\",\n",
    "    \"Infinity\",\n",
    "    \"Mahalanobis2\"\n",
    "]\n",
    "\n",
    "norm = dict(zip(norm_names, [\n",
    "    lambda x: np.sum(x**2, axis=0, keepdims=True),\n",
    "    lambda x: np.sum(np.abs(x), axis=0, keepdims=True),\n",
    "    lambda x: np.max(np.abs(x), axis=0, keepdims=True),\n",
    "    lambda S: lambda x: np.concatenate(\n",
    "        [c[None, :] @ inv(S) @ c[:, None] for c in x.T])\n",
    "]))\n",
    "\n",
    "def dist_matrix(X, Y, norm=norm[\"Euclidean2\"]):\n",
    "    D = np.zeros((X.shape[1], Y.shape[1]))\n",
    "    for x, y in iter.product(range(X.shape[1]), range(Y.shape[1])):\n",
    "        D[x, y] = norm(X[:, x] - Y[:, y])\n",
    "    return D\n",
    "\n",
    "\n",
    "def compare_cluster(U, Ud):\n",
    "    D = dist_matrix(U.T, Ud.T, norm[\"Manhattan\"])\n",
    "    return U[np.argmin(D, axis=0), :]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_names = [\"Uniform\", \"Triangular\", \"Normal\"]\n",
    "\n",
    "\n",
    "def pdf(pdf, x, N):\n",
    "    return pd.Series(dict(zip(pdf_names, [\n",
    "        sci.stats.uniform.pdf(x, 0, N),\n",
    "        sci.stats.triang.pdf(x, 0.5, -1, N+1),\n",
    "        sci.stats.norm.pdf(x, N//2, N//6)])\n",
    "    )[pdf], index=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Learner measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(U, Ud):\n",
    "    C_N, X_N = U.shape\n",
    "    CM = np.zeros((C_N, C_N))\n",
    "    U, Ud = np.argmax(U, axis=0), np.argmax(Ud, axis=0)\n",
    "\n",
    "    for x in range(X_N):\n",
    "        CM[U[x], Ud[x]] += 1\n",
    "    return CM\n",
    "\n",
    "# lower bound to sample size\n",
    "def PAC_eta(H_norm, delta, epsilon):\n",
    "    return (np.log(H_norm) - np.log(delta))/epsilon\n",
    "\n",
    "# lower bound to generalization error\n",
    "def PAC_delta(H_norm, epsilon, eta):\n",
    "    return H_norm/np.exp(eta*epsilon)\n",
    "\n",
    "H = 1\n",
    "plt.figure()\n",
    "X = Y = np.linspace(0.1, 1, 10)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "ax = plt.axes(projection='3d', title=f'PAC (H={H})')\n",
    "ax.plot_surface(X, Y, PAC_eta(H, X, Y))\n",
    "set_axis_labels(ax, '$\\\\delta$', '$\\\\varepsilon$', '$\\\\eta$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Retrieval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cluster:\n",
    "    file = 'iris.csv'\n",
    "    index_col, header = 0, 0\n",
    "else:\n",
    "    file = 'examen.csv'\n",
    "    index_col, header = None, None\n",
    "data = pd.read_csv(file, index_col=index_col, header=header).reset_index(drop=True)\n",
    "N_S = len(data.index)\n",
    "indices = data.index\n",
    "axes_names = data.columns.to_list()\n",
    "\n",
    "X_S, Y_S = data.iloc[:, :-1], data.iloc[:, -1]\n",
    "X_S, denormalize_X = normalize(X_S.to_numpy().T)\n",
    "\n",
    "if cluster:\n",
    "    Y_S, cluster_names = name_to_int(Y_S)\n",
    "    Y_S = vectorize(Y_S.to_list())\n",
    "else:\n",
    "    Y_S, denormalize_Y = normalize(Y_S.to_numpy().T)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distribution Comparison (PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = plt.axes(title=\"Probability Density Functions\")\n",
    "for name in pdf_names:\n",
    "    F = pdf(name, indices, N_S)\n",
    "    ax.plot(indices, F, label=name)\n",
    "plt.legend()\n",
    "set_axis_labels(ax, \"Sample Index\", \"$f(x)$\")\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.axes(title=\"Information content\")\n",
    "for name in pdf_names:\n",
    "    F = pdf(name, indices, N_S)\n",
    "    I = np.log2(1/F)\n",
    "    H = np.sum(-F*np.log2(F))\n",
    "    plt.plot(indices, I, label=f\"{name}: {H:.2f}\")\n",
    "plt.legend(title=\"Entropy $H(X)$\")\n",
    "set_axis_labels(ax, \"Sample Index\", \"$I(x)$\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Distribution Comparison (Sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sets = {\"Train\": int(0.6*N_S),\n",
    "        \"Test\": int(0.2*N_S),\n",
    "        \"Validation\": int(0.2*N_S)}\n",
    "\n",
    "sets_S = sets.copy()\n",
    "\n",
    "for name in pdf_names:\n",
    "    weights = pdf(name, indices, N_S)\n",
    "    sets_curr = sets.copy()\n",
    "    plt.figure()\n",
    "    ax = plt.axes(title=f\"{name} Sampling\")\n",
    "    \n",
    "    for k, v in sets_curr.items():\n",
    "        sample = np.random.choice(weights.index, size=v, p=weights/weights.sum(), replace=False)\n",
    "        sets_curr[k] = sample\n",
    "        if name == \"Uniform\":\n",
    "            sets_S[k] = (X_S[:, sample], Y_S[:, sample])\n",
    "        weights = weights.drop(sample)\n",
    "\n",
    "    ax.hist(sets_curr.values(), stacked=True, bins=10,\n",
    "        label=[f\"{k}: {len(v)}\" for k, v in sets_curr.items()]) \n",
    "    set_axis_labels(ax, 'Index', 'Sample Count')   \n",
    "    plt.legend(title=\"Samples per set\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x): return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "def dsoftmax(x): return softmax(x) * (np.eye(x.shape[0]) - softmax(x).T)\n",
    "def sigmoid(x): return expit(x)\n",
    "def dsigmoid(x): return sigmoid(x) * (1 - sigmoid(x))\n",
    "\n",
    "\n",
    "def feedforward(X, phi, dphi, W, B=None, cluster=False, Yd = None, full=False):\n",
    "    k = len(W) - 1\n",
    "    V = [None] * (k+1)\n",
    "    Y = [None] * (k+1)\n",
    "    Y[0] = X\n",
    "    for l in range(1, k+1):\n",
    "        V[l] = W[l] @ Y[l-1]\n",
    "        if B != None:\n",
    "            V[l] += B[l]\n",
    "        Y[l] = phi(V[l])\n",
    "    if cluster:\n",
    "        Y[k] = softmax(V[k])\n",
    "    if Yd is None:\n",
    "        E = 0\n",
    "    else:\n",
    "        dE = Y[k] - Yd\n",
    "        E = np.average(norm[\"Euclidean2\"](Y[k] - Yd))/2\n",
    "    if full:\n",
    "        return Y, V, E, dE\n",
    "    else:\n",
    "        return Y[k], E\n",
    "    \n",
    "\n",
    "def update(X, Yd, phi, dphi, W, B=None, cluster=False, eta=1):\n",
    "    p = X.shape[1]\n",
    "    k = len(W) - 1\n",
    "\n",
    "    # feedforward\n",
    "    Y, V, E, dE = feedforward(X,phi, dphi, W, B, cluster, Yd, full=True)\n",
    "    # backpropagation\n",
    "    delta = [None] * (k+1)\n",
    "    if cluster:\n",
    "        delta[k] = np.concatenate([dsoftmax(V[k][:, [c]]) @ dE[:, [c]]\n",
    "            for c in range(p)], axis=1)\n",
    "    else:\n",
    "        delta[k] = dphi(V[k]) * dE\n",
    "\n",
    "    for l in reversed(range(1, k)):\n",
    "        delta[l] = (W[l+1].T @ delta[l+1]) * dphi(V[l])\n",
    "\n",
    "    # update\n",
    "\n",
    "    for l in range(1, k+1):\n",
    "        W[l] -= eta * (delta[l] @ Y[l-1].T) / p\n",
    "        if B != None:\n",
    "            B[l] -= eta * (delta[l] @ np.ones((p, 1))) / p\n",
    "    return delta, E, W, B\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Yd = sets_S[\"Train\"]\n",
    "X_test, Yd_test = sets_S[\"Test\"]\n",
    "neurons = [X.shape[0], 2, 2, Yd.shape[0]]\n",
    "k = len(neurons) - 1\n",
    "epochs = 1000\n",
    "\n",
    "W = [None] * (k+1)\n",
    "B = [None] * (k+1)\n",
    "for l in range(1, k+1):\n",
    "    W[l] = np.random.randn(neurons[l], neurons[l-1])\n",
    "    B[l] = np.random.randn(neurons[l])[:, None]\n",
    "\n",
    "delta = []\n",
    "E = {\"Train\": [],\n",
    "    \"Test\": [],\n",
    "    \"Validation\": []}\n",
    "Y_test = []\n",
    "epochs = 0\n",
    "delta_curr = np.inf\n",
    "while epochs < 1000:\n",
    "    Y_test_curr, E_test_curr = feedforward(\n",
    "        X_test, sigmoid, dsigmoid, W, B, cluster, Yd_test)\n",
    "    Y_test.append(Y_test_curr)\n",
    "    E[\"Test\"].append(E_test_curr)\n",
    "    \n",
    "    delta_curr, E_train, W, B = update(X, Yd, sigmoid, dsigmoid, W, B, cluster, 1)\n",
    "    delta_curr = [np.mean(norm[\"Euclidean2\"](delta_curr[l]))\n",
    "                     for l in range(1, k+1)]\n",
    "    delta.append(delta_curr)\n",
    "    E[\"Train\"].append(E_train)\n",
    "    epochs += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "ax = plt.axes(title=f\"Gradients per epoch\")\n",
    "gradients = [f\"{l}: {d:.2e}\" for l, d in enumerate(delta[-1])]\n",
    "ax.plot(range(epochs), delta, label=gradients)\n",
    "ax.legend(title=\"Layer\")\n",
    "set_axis_labels(ax, \"Epoch\", \n",
    "    \"Average Length of Gradients $\\\\overline{\\\\Vert\\\\delta_l\\\\Vert_2}$\")\n",
    "\n",
    "plt.figure()\n",
    "ax = plt.axes(title=f\"Error per epoch\")\n",
    "ax.plot(range(epochs), E[\"Train\"], label=f\"Train: {E['Train'][-1]:.2e}\")\n",
    "ax.plot(range(epochs), E[\"Test\"], label=f\"Test: {E['Test'][-1]:.2e}\")\n",
    "plt.legend(title=\"Set\")\n",
    "set_axis_labels(ax, \"Epoch\",\n",
    "    \"Average Error $\\\\frac{\\\\overline{\\\\Vert Y_d - Y_k\\\\Vert_2}}{2}$\")\n",
    "\n",
    "if cluster:\n",
    "    for name, XY_S in sets_S.items():\n",
    "        U, _ = feedforward(XY_S[0], sigmoid, dsigmoid, W, B, cluster=True)\n",
    "        plot_cluster(XY_S[0], U, Ud=XY_S[1],\n",
    "            title=f\"Model Output ({name})\",\n",
    "            cluster_names=cluster_names[:3], axes_names=axes_names[:3])\n",
    "else:\n",
    "    plot_N = 11\n",
    "    ticks = np.linspace(1, len(Y_test), plot_N, dtype=int)\n",
    "    plt.figure()\n",
    "    ax = plt.axes(projection=\"3d\", title=f\"Output (Test)\")\n",
    "    ax.scatter(*X_test[:2, :], Yd_test, label=\"Reference\", color='darkgreen')\n",
    "    plt.legend()\n",
    "    cmap = plt.get_cmap(\"inferno_r\")\n",
    "    cnorm = mpl.colors.Normalize(vmin=np.min(ticks), vmax=np.max(ticks))\n",
    "    for c in ticks-1:\n",
    "        ax.scatter(*X_test[:2, :], Y_test[c][0], color = cmap(cnorm(c)), alpha=0.5)\n",
    "  \n",
    "    sm = plt.cm.ScalarMappable(cmap=cmap, norm=cnorm)\n",
    "    cbar = plt.colorbar(sm, ticks=ticks, ax=ax,\n",
    "                        location=\"bottom\", label=\"Epoch\")\n",
    "    set_axis_labels(ax, \"$X_0$\", \"$X_1$\", \"$Y_0$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost(X, U, C, norm=norm[\"Euclidean2\"]):\n",
    "    return np.sum(norm(X - C[:, np.argmax(U, axis=0)]))\n",
    "\n",
    "\n",
    "def update_clusters(X, C, norm=norm[\"Euclidean2\"]):\n",
    "    U = np.zeros((C.shape[1], X.shape[1]), dtype=int)\n",
    "    D = dist_matrix(C, X, norm)\n",
    "    I = np.identity(C.shape[1])\n",
    "    for x, c in enumerate(np.argmin(D, axis=0)):\n",
    "        U[:, x] = I[:, c]\n",
    "    return U\n",
    "\n",
    "def update_centers(X, U):\n",
    "    C = np.zeros((X.shape[0], U.shape[0]))\n",
    "    for c, u in enumerate(U):\n",
    "        C[:, c] = np.average(X[:, u == 1], axis=1)\n",
    "    return C\n",
    "\n",
    "\n",
    "def k_means(k, X, epsilon, norm=norm[\"Euclidean2\"]):\n",
    "    C = X[:, np.random.randint(0, X.shape[1], size=k)]\n",
    "    J, J_new = np.inf, 0\n",
    "    while np.abs(J - J_new) > epsilon:\n",
    "        J = J_new\n",
    "        U = update_clusters(X, C)\n",
    "        C = update_centers(X, U)\n",
    "        J_new = cost(X, U, C, norm)\n",
    "    return update_clusters(X, C), C\n",
    "\n",
    "U, C = k_means(3, X, 0.002)\n",
    "for name, XY_S in sets_S.items():\n",
    "    U = update_clusters(XY_S[0], C)\n",
    "    plot_cluster(XY_S[0], compare_cluster(U, XY_S[1]), Ud=XY_S[1], C=C,\n",
    "        title=f\"K-means ({name})\", \n",
    "        cluster_names=cluster_names[:3], axes_names=axes_names[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fuzzy C-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cost_fuzzy(X, U, C, m, norm=norm[\"Euclidean2\"]):\n",
    "    D = dist_matrix(C, X, norm)\n",
    "    return np.sum((U[:, :]**m) * D[:, :])\n",
    "\n",
    "def update_centers_fuzzy(X, U, m):\n",
    "    C = np.zeros((X.shape[0], U.shape[0]))\n",
    "    Um = U**m\n",
    "    for c in range(C.shape[1]):\n",
    "        C[:, c] = np.sum(Um[c, :] * X[:, :], axis=1) / np.sum(Um[c, :])\n",
    "    return C\n",
    "\n",
    "\n",
    "def update_clusters_fuzzy(X, C, m, norm=norm[\"Euclidean2\"]):\n",
    "    D = dist_matrix(C, X, norm)**(1/(m-1))\n",
    "    U = np.zeros((C.shape[1], X.shape[1]))\n",
    "    for x in range(X.shape[1]):\n",
    "        U[:, x] = 1/(D[:, x] * np.sum(1/D[:, x]))\n",
    "    return U\n",
    "\n",
    "def fuzzy_c_means(c, X, m, epsilon):\n",
    "    U = np.random.random((c, X.shape[1]))\n",
    "    U /= np.sum(U, axis=0)\n",
    "    C = update_centers_fuzzy(X, U, m)\n",
    "    J, J_new = np.inf, 0\n",
    "    while np.abs(J - J_new) > epsilon:\n",
    "        J = J_new\n",
    "        C = update_centers_fuzzy(X, U, m)\n",
    "        U = update_clusters_fuzzy(X, C, m)\n",
    "        J_new = cost_fuzzy(X, U, C, m)\n",
    "    return U, C\n",
    "\n",
    "m = 2\n",
    "U, C = fuzzy_c_means(3, X, m, 0.002)\n",
    "for name, XY_S in sets_S.items():\n",
    "    U = update_clusters_fuzzy(XY_S[0], C, m)\n",
    "    plot_cluster(XY_S[0], compare_cluster(U, XY_S[1]), Ud=XY_S[1], C=C,\n",
    "                 title=f\"Fuzzy C-means ({name})\",\n",
    "                 cluster_names=cluster_names[:3], axes_names=axes_names[:3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mountain Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mountain(X, sigma, n, norm=norm[\"Euclidean2\"]):\n",
    "    def m(v): return np.sum(np.exp(-norm(X-v)/(2 * sigma**2)))\n",
    "    k = 3\n",
    "    mesh = list(iter.product(*[np.linspace(0, 1, n) for _ in range(X.shape[0])]))\n",
    "    C = set(mesh)\n",
    "    mC = {c: m(np.array(c)[:, None]) for c in mesh}\n",
    "    for c in mesh:\n",
    "        print(c)\n",
    "    while len(C) > k:\n",
    "        print(mesh)\n",
    "        a = max(mC, key=mC.get)\n",
    "        print(a, mC[a])\n",
    "\n",
    "mountain(X, 1, 3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e5b669eafd9a1d83afd0a84e8d3675c3b9ad744df943aeb108be405a696fcc51"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
