\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote.
% If that is unneeded, please comment it out.
\usepackage[backend=bibtex,style=ieee,natbib=true]{biblatex} 
\bibliography{references.bib}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithm2e}
\usepackage{graphicx}
\graphicspath{ {../images/} }
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{todonotes}

\begin{document}

\title{Automated Labeling of Pulsar Star Candidates}

\author{\IEEEauthorblockN{Andrés Felipe Ortega Montoya}
    \IEEEauthorblockA{\textit{Mathematical Engineering} \\
        Medellín, Colombia \\
        aortega7@eafit.edu.co}}

\maketitle

\begin{abstract}
    \todo{}
\end{abstract}

\begin{IEEEkeywords}
    AI, Pulsar, Labeling
\end{IEEEkeywords}

\section{Introduction\label{sec:intro}}
It is know that since ancient times humankind humankind has made an effort to
catalogue the stars visible in the sky, from the $36$ stars listed in the
\"three stars each\" by the ancient Babylonians in the $2000s\ B.C.$
\cite{astronomy:history:north}; to the last full-sky catalogue to date with over
$1.5$ billion objects, the Gaia Data Release 3 \cite{gaia:dr3:esa}, made available
to the public on June 13, 2022, by the European Space Agency.

With the advent of modern telescopes and satellites, the capability to detect
even the dimmest lights from space have reached heights previously unthinkable.
The gaia catalogue is but one of the many catalogues available to date, which
serves to illustrate sheer quantity of data currently collected. Current
telescopes are constantly surveying the sky in search for evermore objects to study.

This massive volume of data means that most of it will never get to be studied
by hand by astronomers, instead better focusing their efforts studying object
that are already known to possess qualities that may result of special interest.
In order to identify these special observations from the sea of data available,
multiple techniques have been developed \cite{pulsar:dataset:explanation:lyon}.
In the current paper, data from a specialized pulsar catalogue
\cite{pulsar:dataset:lyon}, is approached using multiple Artificial Intelligence
techniques in order to identify possible pulsar star candidates.
\todo{Sections list and definition}

\section{The Problem\label{sec:problem}}
The name Pulsar (from \underline{Puls}ating st\underline{ar}) refers to rapidly
rotating and strongly magnetized neutron stars \cite{pulsar:definition:nasa}.
The magnetic field present in these stars, accelerates particles to high
speeds around the star, which may ultimately get thrown into the magnetic
poles poles of the pulsar, heating them. This heat gradient gets to be so high
that the poles may act as hotspots, radiating massive amounts of heat into space,
distinctively more than that of the rest of the star. As the star spins,
these beams of electromagnetic waves (mostly on the X-ray spectrum) go in and out
of view from earth at extremely regular periods, motion that makes it appear as
pulsating in the sky, thus, giving it its name.

Pulsars get to occupy an special place in solving a wide range of physical and
astronomical problems \cite{pulsar:importance:kramer}. The extreme conditions on
these allow to test the limits of gravitational theories, solid state physics and
plasma physics under extreme conditions among others. For most uses one needs
not to know how pulsars work but treating them as natural clocks with stabilities
similar to the best atomic clocks over time spans of months or years.
Correctly identifying these objects presents then a challenge worth approaching.

Although over short periods of time, the signals received from each pulsar
varies slightly on each rotation \cite{pulsar:importance:kramer}, it is possible
to make an appropriate labeling using average measures over longer periods of time.
In practice however, their dim signals means that almost all detections are a
result of radio frequency interference and noise \cite{pulsar:dataset:explanation:lyon}.
creating the necessity of finding new ways to differentiate real candidates
from the rest.

\citet{pulsar:dataset:lyon} provides a dataset which contains a total of $17898$
data points, $1639$ for them corresponding to real pulsar detections (verified
by human annotators), and $16259$ detections associated to RFI or noise.
The dataset was modified and split in Kaggle \cite{pulsar:dataset:kaggle} into one
unlabelled set with $5370$ observations and a labelled set with $12528$ observations,
the latter being the one used on this paper for purposes of training, testing,
and validation of the techniques used.

As pulsars appear very dimmly in the sky, it is necessary to collect a few hundred
to thousand of pulses together in order to discern them from the background noise,
this collection is \textit{folded} into an integrated pulse profile
\cite{pulsar:importance:kramer}. The dataset previously mentioned
has 8 continuous variables per pulsar candidate, each with its corresponding
binary labeling. The first $4$ of them are statistics that describe the
integrated pulse profile of the star, while the last $4$ are similarly
statistics from the DM-SNR (Dispersion Measure - Signal to Noise Ratio)
\cite{pulsar:dataset:explanation:lyon}. The task being then to decide based
on these 8 variables if any given object is a possible pulsar or not.

In the order that they appear on the database, the variables are:

\begin{itemize}
    \item Mean of the integrated profile.
    \item Standard deviation of the integrated profile.
    \item Excess kurtosis of the integrated profile.
    \item Skewness of the integrated profile.
    \item Mean of the DM-SNR curve.
    \item Standard deviation of the DM-SNR curve.
    \item Excess kurtosis of the DM-SNR curve.
    \item Skewness of the DM-SNR curve.
    \item Class 
\end{itemize}


\section{State of the art\label{sec:state_of_art}}

\todo{}

\section{Sampling\label{sec:sample}}

With any proposed model the goal is to best approximate certain dynamic,
having no more at one's disposal that any observations of such event. The recolection
of many of these measures is called a sample. 
Before applying any technique that to tackle the problem at hand, one may
first decide how to construct an appropiate sample that best represents the
desired phenomenom.

For the current problem we desire to have a representation
of equal proportionality of the labels over the dataset and any sample of it.
Furthermore the data is to be split on $3$ mutually exclusive sets, one for
training the models with a $60\%$ of the original data; and a test and validation
sets with $20\%$ each. These last $2$ sets will serve to observe the generability
capacities of the models proposed.

The algorithm used to construct these samples is Algorithm~\ref{alg:sample}.
In this it can be seen that the construction of the sample sets is done
sequencially, meaning that each set will not have available any of the previously
selected data and any bias introduced by the sampling method may propagate and
increase further.

\begin{algorithm}
    $P = \{0.6,\ 0.2,\ 0.2\}$\\
    $P' \gets \{\}$\\
    $I \gets \{x:\ 0\leq x\leq \lvert S\rvert,\ x\in\mathbb{N}\}$\\
    \For{$p \in P$}{
        $F(x) \gets \text{PDF} \textbf{ with support } I$\\
        $s \gets p\lvert S\rvert \textbf{ realizations from } F(x)$\\
        $P' \gets P' \cup \{s\}$\\
        $I \gets I - s$\\
    }
    \caption{Sampling algorithm}\label{alg:sample}
\end{algorithm}

The sampling for the current paper was made indexing all of the data with
sequencial natural numbers starting from $0$ and sampling those instead.
To avoid the possible case of introducing bias on the samples, $3$ sampling
distributions where chosen to compare their results, these being a uniform
triangular, and normal distributions. As the number of observations is finite,
each index $x$ is given a probability of $f(x)$ (with $f$ being the PDF) 
scaled so that the sum of the PDF over all indices equals $1$. This accomodation
is made in order to use indistinctively discrete or continuous distributions.

For a sample size $N$, the before mentioned sampling for each distribution is
made as follows:
\begin{itemize}
    \item Uniform: spans from $0$ to $N$
    \item Triangular: mode of $N/2$, spans from $-1$ to $N+1$ so that indices
        on the extremes get a non zero probability
    \item Normal: mean of $N/2$, standard deviation of $N/6$
\end{itemize}

Each distribution has a corresponding self-information function that measures
the "surprisal" of any outcome. The expected information of a given probability
distribution is called entropy \cite{information:borda}. Under these definitions
one could argue that a distribution with a higher entropy corresponds to a
better sampling technique, one more informative. Figure \ref{fig:sample:info}
presents then the respective PDF's over the indices, with their corresponding
self-information and entropy, from which we can infer that uniform sampling is
most desired.


\begin{figure}[h]
    \includegraphics[width=\linewidth]{sample:info}
    \caption{Example of a figure caption. \label{fig:sample:info}}
\end{figure}

With the motivation of contrasting the previously obtained results numerically;
a histogram of every sample is made, higlighting the distribution of the selected
data over the different sets and the representation of the desired labels in each
of them as seen in Figure \ref{fig:sample:hist}. 

\todo{explain histograms}

When filling each of our sets as described in Algorithm \ref{alg:sample}, 
for the $N$ realizations generated from the desired distribution, they are document
without replacement. This is to comply with the definition of the $3$ sets As
mutually exclusive. The process of sampling without replacement involves
that the index realized is removed from the support of the distribution and the
remaining probabilities are normalized once more to generate the next realization.


\begin{figure}[h]
    \includegraphics[width=\linewidth]{sample:dist:train}
    \caption{Slice of the training data distribution. \label{fig:sample:dist:train}}
\end{figure}

\begin{figure*}[h]
    \includegraphics[width=\linewidth]{sample:hist}
    \caption{Example of a figure caption. \label{fig:sample:hist}}
\end{figure*}


\printbibliography
\end{document}
