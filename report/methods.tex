\subsection{Visualization}

In order to have a better idea of the disposition of the space,
Figure \ref{fig:pair} has a projection of every pair of variables, while
Figure \ref{fig:umap} contains a UMAP projection of the data.

\begin{figure*}[ht]
    \includegraphics[width=\linewidth]{pair}
    \caption{Pairwise projection of the data. \label{fig:pair}}
\end{figure*}

\begin{figure}[ht]
    \includegraphics[width=\linewidth]{umap}
    \caption{UMAP Projection of the data. \label{fig:umap}}
\end{figure}

While every pairwise projecttion shows a clear separation between classes, the
UMAP projection shows the possitive class appended to the tail of the negative
class. This might imply that although there is a clear distiction of the general
class, the border between them is fuzzy and might be harder to pin down.

\subsection{Sampling process\label{sec:sample}}

With any proposed model the goal is to best approximate certain dynamic, having
but some samples of such event. Before applying any technique to tackle the
problem at hand, one may first decide how to construct an appropiate training
sample that best represents the desired phenomenom so as not to overfit the
model while retaining the dynamic of the system.

This sampling is done under a random distribution and it is usually desired to
have an equal representation of the labels in the sample than that present in
the original dataset. This being said, the data is to be splitted on $3$
mutually exclusive sets, one for training the models with $60\%$ of the original
data; and a test and validation sets with $20\%$ each. These last $2$ sets will
serve to observe the generability capacities of the models proposed.

The sampling for the current document was made by sampling instead an indexing
of the data with sequencial natural numbers starting from 0. To avoid the
possible case of introducing bias on the samples, $3$ sampling distributions
where chosen in order to compare their results and choose the best performing
one. The selected distributions being uniform, triangular, and normal
distributions. As the number of observations is finite, for continous
distributions each index $x$ is given a probability of $f(x)$ (with $f$ being
the PDF) and then it is scaled so that the sum of the probability over all
indices equals $1$. This accomodation is made in order to use indistinctively
discrete or continuous distributions.

For a sample size $N$, the parameters for the before mentioned distributions 
are chosen as follows:
\begin{itemize}
    \item Uniform: spans from $0$ to $N$
    \item Triangular: mode of $N/2$, spans from $-1$ to $N+1$ so that indices on
        the extremes get a non zero probability
    \item Normal: mean of $N/2$, standard deviation of $N/6$
\end{itemize} 

Each distribution has a corresponding self-information function that measures
the ``surprisal'' of any outcome. The expected information of a given
probability distribution is called entropy~\cite{information:borda}. Under these
definitions one could argue that a distribution with a higher entropy
corresponds to a better sampling technique, one more informative.
Figure~\ref{fig:sample:info} then presents the probabily distributions selected
with their corresponding self-information and entropy.

\begin{figure}[ht]
    \includegraphics[width=\linewidth]{sample:info}
    \caption{Analytical properties of various PDF's. \label{fig:sample:info}}
\end{figure}

In the Algorithm~\ref{alg:sample} used to construct the sample groups, it can be
seen that the construction is done sequencially, meaning that each set will not
have available any of the previously selected data and, any bias introduced by
the sampling method may propagate to other groups.

\begin{algorithm}[ht]
    $P = \{0.6,\ 0.2,\ 0.2\}$\\
    $P' \gets \{\}$\\
    $I \gets \{x:\ 0\leq x\leq \lvert S\rvert,\ x\in\mathbb{N}\}$\\
    \For{$p \in P$}{ $f(x) \gets \text{PDF} \textbf{ with support } I$\\
        $s \gets p\lvert S\rvert \textbf{ realizations from } f(x)$\\
        $P' \gets P' \cup \{s\}$\\
        $I \gets I - s$\\
    } \caption{Sampling algorithm\label{alg:sample}}
\end{algorithm}

When filling each of the sets as described in the previous algorithm, the $N$
realizations from the desired distribution are generated without replacement;
this is to comply with the definition of the $3$ being mutually exclusive. The
process of sampling without replacement involves that the indices generated are
removed from the support of the distribution and the probability for the
remaining points are normalized once again for the next realization.

With the motivation of contrasting the previously obtained results numerically;
a histogram of a sample from every PDF is made, higlighting the distribution of
the selected data over the different groups and, the representation of the
desired labels in each of them as seen in Figure~\ref{fig:sample:hist:index}.

\begin{figure*}[ht]
    \includegraphics[width=\linewidth]{sample:hist:index}
    \caption{Distribution of indices over data sets. \label{fig:sample:hist:index}}
\end{figure*}

\subsection{Final sample disposition\label{sec:sample:final}}

As could be seen in Section \ref{sec:sample} with Figure \ref{fig:sample:info}
the uniform distribution results on the optimal entropy value; furthermore, as
expected, it results in an equal representation of all indices over the $3$ of
the desired sets. Contrasting this, it is possible to note that for both the
triangular and normal sampling there is an over-representation of the central
objects in the training set, while having an under-representation of these for
the training and validation sets, which may lead to an scenario where what the
model is trained for has little to no relation with what it is being tested
against.

However, regardless of the sampling PDF selected, the distribution of the labels
over all $3$ sets remains consistent, which may be an indicator supporting that the
dataset was previously scrambled or that the process of collecting the data is
uniform in nature.

This being said, the uniform sampling was selected as the distribution of
choice. Then, in order to better have a better idea of how the data is
distributed over the sets, in Figure~\ref{fig:sample:hist:variables} it is shown
how each of the variables previously mentioned in \ref{tab:variables} are
represented in the partitions. Although we can see that all variables are
properly sampled in the partitioned sets, it is interesting to point out that
variables $X_3, X_4$ and $X_7$ seem to be extremely bunched up into a single
value, with a few outliers far from this point. 

\begin{figure*}[ht]
    \includegraphics[width=\linewidth]{sample:hist:variables}
    \caption{Distribution of input variables over sampling \label{fig:sample:hist:variables}}
\end{figure*}

Before any sampling is done, and not to incur in any special treatment with
missing data, from the initial labelled dataset with $12528$ entries, $3255$ of
them where dropped for having empty entries for any variable. 

This being said, after sampling and for the purpose of better training all
models, every variable is normalized linearly into a value in the range $[0,1]$;
and for the output, 2 artificial and opposite boolean variables were created,
$Y_0$ and $Y_1$, where $Y_0$ is $1$ when the candidate is a pulsar, and $Y_1$ is
$1$ when it is not. This makes all the sample space and every possible
observation on it a point in a unitary hypercube.

The problem previously described for variables $X_3, X_4$ and $X_7$ is not going
to be tackled in the current work, although it could be approaced by using a
logarithmic normalization instead of a linear one so as to spread the central
spike and group the outliers on the extrema of the variable range (that being
$0$ or $1$).