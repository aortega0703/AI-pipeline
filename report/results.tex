Figures \ref{fig:mountain} and \ref{fig:substractive} show the result of running
mountain clustering and substractive clustering on the training data. These
algorithms are designed to find how many (and where) cluster centers are
present to then assign cluster membership according with minimum
distance. For the present problem both methods suggested the appeareance of 3
cluster centers which may suggest the appeareance of another object of interest
in the data.

\begin{figure}[t]
    \includegraphics[width=\linewidth]{mountain.png}
    \caption{Cluster centers and membership using mountain clustering
    \label{fig:mountain}}
\end{figure}
\begin{figure}[t]
    \includegraphics[width=\linewidth]{substractive.png}
    \caption{Cluster centers and membership using substractive clustering
    \label{fig:substractive}}
\end{figure}

After running K-means and fuzzy C-means on the dataset with $3$ clusters each
(as suggested by the previous algorithms), it is possible to note that the
number of members of the third new group rises considerably, although still
staying much smaller than the other $2$.

\begin{figure}[t]
    \includegraphics[width=\linewidth]{kmeans.png}
    \caption{Cluster centers and membership using K-means with K=3
    \label{fig:kmeans}}
\end{figure}
\begin{figure}[t]
    \includegraphics[width=\linewidth]{fuzzycmeans.png}
    \caption{Cluster centers and membership using Fuzzy C-means with C=3 
    \label{fig:fuzzycmeans}}
\end{figure}

\subsection{Performance}
In order to evaluate the performance of each of the clustering methods, the 
Daviesâ€“Bouldin index, and Dunn index were run on the results obtained. The
Silhouette index was implemented but not run on these as the time complexity
($O(N^2)$ with $N$ the number of points) is prohibitively costly. 

For the DB index an smaller value indicates a better clustering while the Dunn
index puts the better consideration on the highest value. Both indices measure
the ratio of intra cluster distance with intercluster distance, although in 
different ways.

\begin{table}
    \begin{tabular}{l|lllll}
             & Mountain & Substractive & K-means & C-means & DBSCAN\\\hline
        DB   & 0.3064 & 0.3701 & 1.1039 & 1.0938 & 3.4720\\
        Dunn & 0.1516 & 0.1783 & 0.0158 & 0.0161 & 0.0170
    \end{tabular}
\end{table}

\begin{table}
    \begin{tabular}{l|lllll}
             & Mountain & Substractive & K-means & C-means & DBSCAN\\\hline
        DB   & 0.3178 & 0.3904 & 1.0164 & 1.0028 & 3.6715\\
        Dunn & 0.1618 & 0.1740 & 0.0175 & 0.0181 & 0.0189
    \end{tabular}
\end{table}

\begin{table}
    \begin{tabular}{l|lllll}
             & Mountain & Substractive & K-means & C-means & DBSCAN\\\hline
        DB   & 0.2939 & 0.3820 & 1.0651 & 1.0618 & 3.5887\\
        Dunn & 0.1574 & 0.1651 & 0.0146 & 0.0149 & 0.0200
    \end{tabular}
\end{table}

% \begin{table*}
%     \csvreader[
%         centered tabular=l|ccccccccccc,
%         column count=12,
%         no head,
%         late after first line = {\\\hline},
%     ]{../tables/index:train.csv}{}%
%     {\csvlinetotablerow}%
%     \caption{Multiple indices on training data\label{tab:index:train}}
% \end{table*}

% \begin{table*}
%     \csvreader[
%         centered tabular=l|ccccccccccc,
%         column count=12,
%         no head,
%         late after first line = {\\\hline},
%     ]{../tables/index:test.csv}{}%
%     {\csvlinetotablerow}%
%     \caption{Multiple indices on testing data\label{tab:index:test}}
% \end{table*}

% \begin{table*}
%     \csvreader[
%         centered tabular=l|ccccccccccc,
%         column count=12,
%         no head,
%         late after first line = {\\\hline},
%     ]{../tables/index:validation.csv}{}%
%     {\csvlinetotablerow}%
%     \caption{Multiple indices on validation data\label{tab:index:validation}}
% \end{table*}