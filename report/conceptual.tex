\subsection{Supervised learning\label{sec:supervised}} 

Supervised learning is called when both inputs and outputs for a given problem
are used in order to mimic the underlying behaviour and simulate its dynamic
\cite{supervised:definition:rusell}. The ouput for our problem is present and is
revised by actual human annotators, so a supervised approach may be a good start
to analyse the data. In the following sections multiple supervised techniques
are described and compared agains multiple indices.

\subsubsection{Linear and logistic regression\label{sec:regression}}

Linear regression is probably the simplest way of tuning parameters to follow
given data. It assumes a matrix $X$ of $n$ measured variables over $p$
observations, a matrix $Y$ with $m$ output variables over the same opulation,
and a matrix $\beta$ relating them. It assumes, as its name implies, that the
dynamic of the model can be expressed as a linear equation of the form: 
%
\begin{equation}\label{eq:linear}
    \overset{(m\times p)}{Y}
    = \overset{(m\times n)}{\beta} \overset{(n\times p)}{X}
\end{equation}
%
Where it is desired to find the best parameter $\beta$. This can be done making
use of the pseudo inverse of $X$ as follows:
%
\begin{align*}
    \overset{(m\times p)}{Y}
        &= \overset{(m\times n)}{\beta} \overset{(n\times p)}{X}\\
    \overset{(m\times p)}{Y}\overset{(p\times n)}{X^T}
        &= \overset{(m\times p)}{\beta X} \overset{(p\times n)}{X^T}\nonumber\\
    \overset{(m\times n)}{Y X^T} \overset{(n\times n)}{\left(X X^T\right)^{-1}}
        &= \overset{(m\times n)}{\beta}
\end{align*}
%
It's important to note that usually a row of $1$s is added to $X$ in order to
account for a constant term in the model. A similar model is logistic
regression, which deals with data in the range $(0, 1)$ that have a great
distinction between then, making it especially useful for problems of
classification. It assumes a model of the form:
%
\begin{equation}
    Y = \frac{1}{1+\exp{(-\beta X)}}
\end{equation}
%
Where addition, division, and exponentiation are defined elementwise; which
imposes multiple and simultaneous sigmoid functions of $1$ output. 
This model can be solved as a linear regression with the substitution:
%
\begin{align*}
    Y &= \frac{1}{1+e^{(-\beta X)}}\nonumber\\
        &= \frac{e^{(\beta X)}}{1+e^{(\beta X)}}\\
    Y+Ye^{(\beta X)}
        &= e^{(\beta X)}\\
    Y &= (1-Y) e^{(\beta X)}\\
    \frac{Y}{1-Y} &= e^{(\beta X)}\\
    \ln{\left(\frac{Y}{1-Y}\right)} &= \beta X\\
        &= P
\end{align*}
%
Using the known output $Y$ to find $P$, solving for $\beta$ in the new linear
model, and substituting everything back into the original expression.

\subsubsection{Multilayer perceptron\label{sec:nn}}
\todo[inline]{nn describe}